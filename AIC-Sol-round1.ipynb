{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class MitosisDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.frame.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG-11 expects 224x224 inputs\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Assuming 'training images' is your directory and 'Train.csv' is your label file\n",
    "# train_dataset = MitosisDataset(csv_file='Train.csv', root_dir='Training Images', transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# # 2. Model Setup\n",
    "# model = models.vgg11(pretrained=True)  # Load a pre-trained VGG-11 model\n",
    "\n",
    "# # Modify the final layer to match the number of classes in your dataset\n",
    "# num_classes = 2  # Mitosis vs Normal\n",
    "# model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # 3. Training\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# for epoch in range(10):  # Loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_loader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 200 == 199:  # Print every 200 mini-batches\n",
    "#             print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 200}')\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "# 4. Evaluation (Optional)\n",
    "# Here, you could load your test dataset and evaluate the model's performance similarly to how training is done, but with no backpropagation or weight update steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     98\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    100\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust labels shape for BCELoss\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[53], line 44\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     41\u001b[0m label_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     43\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Ensure image is RGB\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     47\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the labels\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "\n",
    "# Assuming 'name' column contains image filenames and 'label' column contains the labels\n",
    "image_files = df['Image'].values\n",
    "image_files = [f\"{name}.jpg\" for name in image_files]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {\n",
    "            'Normal': 0,\n",
    "            'Mitosis': 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        label_text = self.labels[idx]\n",
    "        label = self.label_mapping[label_text]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure image is RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label_tensor\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Model architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.float().unsqueeze(1)  # Adjust labels shape for BCELoss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop (optional, for evaluating model performance)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Similar validation loop as training\n",
    "        pass\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Predictions using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "test_image_files = test_df['Image'].values\n",
    "test_image_files = [f\"{name}.jpg\" for name in test_image_files]\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_dataset = TestDataset(test_image_files, 'Testing Images', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m      7\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\densenet.py:213\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 213\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m    214\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\densenet.py:122\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    120\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 122\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m layer(features)\n\u001b[0;32m    123\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\densenet.py:88\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     86\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_function(prev_features)\n\u001b[0;32m     90\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\densenet.py:48\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbn_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 48\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(concated_features)))  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Map numeric predictions back to textual form\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "textual_predictions = [label_mapping_inverse[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with the predictions\n",
    "test_df['Label'] = textual_predictions\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "test_df.to_csv('Test_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the predictions were 'Normal' which makes me thing the training data was biased. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_csv('Train.csv')\n",
    "\n",
    "# Plot the distribution of labels\n",
    "df['Label'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Labels in Train.csv')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, yes. There was biasness. Now, let's re-make the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with image augmentationm weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8741, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 2/10, Loss: 0.7918, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 3/10, Loss: 0.8328, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 4/10, Loss: 0.8323, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 5/10, Loss: 0.7714, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 6/10, Loss: 0.8126, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 7/10, Loss: 0.8120, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 8/10, Loss: 0.7913, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 9/10, Loss: 0.8328, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n",
      "Epoch 10/10, Loss: 0.8126, Train Acc: 0.4167, Train F1: 0.5882, Val Acc: 0.3043, Val F1: 0.4667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load the dataset\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "image_files = [f\"{name}.jpg\" for name in df['Image'].values]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {'Normal': 0, 'Mitosis': 1}\n",
    "encoded_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split the dataset\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset class with augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define transforms with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Model architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor[1])\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_preds, train_targets = [], []\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # For metrics calculation\n",
    "            train_preds += torch.round(torch.sigmoid(outputs)).squeeze().tolist()\n",
    "            train_targets += labels.tolist()\n",
    "        \n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                val_preds += torch.round(torch.sigmoid(outputs)).squeeze().tolist()\n",
    "                val_targets += labels.tolist()\n",
    "        \n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Map numeric predictions back to textual form\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "textual_predictions = [label_mapping_inverse[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with the predictions\n",
    "test_df['Label'] = textual_predictions\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "test_df.to_csv('Test_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\Fatima Azfar/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:13<00:00, 2.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4488, Train Acc: 0.7111, Train F1: 0.6533, Val Acc: 0.7174, Val F1: 0.6286\n",
      "Epoch 2/10, Loss: 0.5309, Train Acc: 0.7944, Train F1: 0.7784, Val Acc: 0.7174, Val F1: 0.5806\n",
      "Epoch 3/10, Loss: 0.4034, Train Acc: 0.8389, Train F1: 0.7914, Val Acc: 0.8696, Val F1: 0.7857\n",
      "Epoch 4/10, Loss: 0.3453, Train Acc: 0.8056, Train F1: 0.7879, Val Acc: 0.8478, Val F1: 0.7407\n",
      "Epoch 5/10, Loss: 0.2446, Train Acc: 0.8389, Train F1: 0.8027, Val Acc: 0.8478, Val F1: 0.6957\n",
      "Epoch 6/10, Loss: 0.4118, Train Acc: 0.8167, Train F1: 0.7898, Val Acc: 0.8478, Val F1: 0.7200\n",
      "Epoch 7/10, Loss: 0.3725, Train Acc: 0.8556, Train F1: 0.8243, Val Acc: 0.8478, Val F1: 0.7200\n",
      "Epoch 8/10, Loss: 0.2964, Train Acc: 0.8667, Train F1: 0.8442, Val Acc: 0.8478, Val F1: 0.7407\n",
      "Epoch 9/10, Loss: 0.2247, Train Acc: 0.8889, Train F1: 0.8734, Val Acc: 0.8043, Val F1: 0.6897\n",
      "Epoch 10/10, Loss: 0.3074, Train Acc: 0.8944, Train F1: 0.8725, Val Acc: 0.6957, Val F1: 0.6111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained DenseNet model\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Modify the classifier for binary classification\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, 1)  # Output layer for binary classification\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the criterion with class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor[1])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function adapted for DenseNet\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_preds, train_targets = [], []\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_preds += torch.round(torch.sigmoid(outputs)).squeeze().tolist()\n",
    "            train_targets += labels.tolist()\n",
    "        \n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        train_f1 = f1_score(train_targets, train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_preds += torch.round(torch.sigmoid(outputs)).squeeze().tolist()\n",
    "                val_targets += labels.tolist()\n",
    "        \n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Assuming 'Test Images' directory and 'Test.csv' with an 'Image' column\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "test_image_files = [f\"{name}.jpg\" for name in test_df['Image'].values]\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_dataset = TestDataset(test_image_files, 'Testing Images', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).squeeze()\n",
    "        predicted_labels = torch.round(preds).cpu().numpy()  # Convert to binary labels\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# Map numeric predictions back to label names\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "predicted_labels_text = [label_mapping_inverse[int(pred)] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with predictions\n",
    "test_df['Label'] = predicted_labels_text\n",
    "\n",
    "# Save the updated DataFrame\n",
    "test_df.to_csv('Test_Preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Submission Result: 70.58% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet with Enhancements\n",
    "Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['67.jpg', '55.jpg', '124.jpg', '127.jpg', '209.jpg', '84.jpg', '93.jpg', '97.jpg', '115.jpg', '38.jpg', '24.jpg', '119.jpg', '183.jpg', '194.jpg', '10.jpg', '69.jpg', '29.jpg', '19.jpg', '108.jpg', '66.jpg', '73.jpg', '5.jpg', '56.jpg', '112.jpg', '137.jpg', '135.jpg', '65.jpg', '140.jpg', '190.jpg', '224.jpg', '31.jpg', '12.jpg', '35.jpg', '28.jpg', '42.jpg', '125.jpg', '113.jpg', '153.jpg', '51.jpg', '181.jpg', '180.jpg', '76.jpg', '41.jpg', '95.jpg', '138.jpg', '78.jpg', '156.jpg', '26.jpg', '205.jpg', '189.jpg', '126.jpg', '0.jpg', '2.jpg', '77.jpg', '46.jpg', '100.jpg', '109.jpg', '136.jpg', '162.jpg', '90.jpg', '85.jpg', '159.jpg', '150.jpg', '98.jpg', '36.jpg', '132.jpg', '61.jpg', '22.jpg', '147.jpg', '158.jpg', '33.jpg', '11.jpg', '220.jpg', '177.jpg', '6.jpg', '27.jpg', '139.jpg', '216.jpg', '217.jpg', '154.jpg', '4.jpg', '122.jpg', '32.jpg', '161.jpg', '62.jpg', '128.jpg', '212.jpg', '170.jpg', '70.jpg', '173.jpg', '64.jpg', '44.jpg', '146.jpg', '40.jpg', '123.jpg', '23.jpg', '168.jpg', '164.jpg', '81.jpg', '39.jpg', '219.jpg', '47.jpg', '94.jpg', '171.jpg', '43.jpg', '145.jpg', '155.jpg', '3.jpg', '105.jpg', '53.jpg', '133.jpg', '203.jpg', '186.jpg', '208.jpg', '49.jpg', '80.jpg', '34.jpg', '7.jpg', '110.jpg', '91.jpg', '83.jpg', '199.jpg', '204.jpg', '89.jpg', '8.jpg', '13.jpg', '59.jpg', '192.jpg', '131.jpg', '17.jpg', '166.jpg', '72.jpg', '196.jpg', '134.jpg', '185.jpg', '206.jpg', '63.jpg', '54.jpg', '107.jpg', '50.jpg', '174.jpg', '222.jpg', '197.jpg', '169.jpg', '58.jpg', '48.jpg', '88.jpg', '21.jpg', '57.jpg', '160.jpg', '218.jpg', '187.jpg', '191.jpg', '129.jpg', '37.jpg', '157.jpg', '211.jpg', '1.jpg', '52.jpg', '149.jpg', '130.jpg', '151.jpg', '103.jpg', '99.jpg', '116.jpg', '87.jpg', '202.jpg', '74.jpg', '214.jpg', '210.jpg', '121.jpg', '225.jpg', '20.jpg', '188.jpg', '71.jpg', '106.jpg', '14.jpg', '92.jpg', '179.jpg', '102.jpg']\n",
      "['67.jpg', '55.jpg', '124.jpg', '127.jpg', '209.jpg', '84.jpg', '93.jpg', '97.jpg', '115.jpg', '38.jpg', '24.jpg', '119.jpg', '183.jpg', '194.jpg', '10.jpg', '69.jpg', '29.jpg', '19.jpg', '108.jpg', '66.jpg', '73.jpg', '5.jpg', '56.jpg', '112.jpg', '137.jpg', '135.jpg', '65.jpg', '140.jpg', '190.jpg', '224.jpg', '31.jpg', '12.jpg', '35.jpg', '28.jpg', '42.jpg', '125.jpg', '113.jpg', '153.jpg', '51.jpg', '181.jpg', '180.jpg', '76.jpg', '41.jpg', '95.jpg', '138.jpg', '78.jpg', '156.jpg', '26.jpg', '205.jpg', '189.jpg', '126.jpg', '0.jpg', '2.jpg', '77.jpg', '46.jpg', '100.jpg', '109.jpg', '136.jpg', '162.jpg', '90.jpg', '85.jpg', '159.jpg', '150.jpg', '98.jpg', '36.jpg', '132.jpg', '61.jpg', '22.jpg', '147.jpg', '158.jpg', '33.jpg', '11.jpg', '220.jpg', '177.jpg', '6.jpg', '27.jpg', '139.jpg', '216.jpg', '217.jpg', '154.jpg', '4.jpg', '122.jpg', '32.jpg', '161.jpg', '62.jpg', '128.jpg', '212.jpg', '170.jpg', '70.jpg', '173.jpg', '64.jpg', '44.jpg', '146.jpg', '40.jpg', '123.jpg', '23.jpg', '168.jpg', '164.jpg', '81.jpg', '39.jpg', '219.jpg', '47.jpg', '94.jpg', '171.jpg', '43.jpg', '145.jpg', '155.jpg', '3.jpg', '105.jpg', '53.jpg', '133.jpg', '203.jpg', '186.jpg', '208.jpg', '49.jpg', '80.jpg', '34.jpg', '7.jpg', '110.jpg', '91.jpg', '83.jpg', '199.jpg', '204.jpg', '89.jpg', '8.jpg', '13.jpg', '59.jpg', '192.jpg', '131.jpg', '17.jpg', '166.jpg', '72.jpg', '196.jpg', '134.jpg', '185.jpg', '206.jpg', '63.jpg', '54.jpg', '107.jpg', '50.jpg', '174.jpg', '222.jpg', '197.jpg', '169.jpg', '58.jpg', '48.jpg', '88.jpg', '21.jpg', '57.jpg', '160.jpg', '218.jpg', '187.jpg', '191.jpg', '129.jpg', '37.jpg', '157.jpg', '211.jpg', '1.jpg', '52.jpg', '149.jpg', '130.jpg', '151.jpg', '103.jpg', '99.jpg', '116.jpg', '87.jpg', '202.jpg', '74.jpg', '214.jpg', '210.jpg', '121.jpg', '225.jpg', '20.jpg', '188.jpg', '71.jpg', '106.jpg', '14.jpg', '92.jpg', '179.jpg', '102.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "----------\n",
      "['67.jpg', '55.jpg', '124.jpg', '127.jpg', '209.jpg', '84.jpg', '93.jpg', '97.jpg', '115.jpg', '38.jpg', '24.jpg', '119.jpg', '183.jpg', '194.jpg', '10.jpg', '69.jpg', '29.jpg', '19.jpg', '108.jpg', '66.jpg', '73.jpg', '5.jpg', '56.jpg', '112.jpg', '137.jpg', '135.jpg', '65.jpg', '140.jpg', '190.jpg', '224.jpg', '31.jpg', '12.jpg', '35.jpg', '28.jpg', '42.jpg', '125.jpg', '113.jpg', '153.jpg', '51.jpg', '181.jpg', '180.jpg', '76.jpg', '41.jpg', '95.jpg', '138.jpg', '78.jpg', '156.jpg', '26.jpg', '205.jpg', '189.jpg', '126.jpg', '0.jpg', '2.jpg', '77.jpg', '46.jpg', '100.jpg', '109.jpg', '136.jpg', '162.jpg', '90.jpg', '85.jpg', '159.jpg', '150.jpg', '98.jpg', '36.jpg', '132.jpg', '61.jpg', '22.jpg', '147.jpg', '158.jpg', '33.jpg', '11.jpg', '220.jpg', '177.jpg', '6.jpg', '27.jpg', '139.jpg', '216.jpg', '217.jpg', '154.jpg', '4.jpg', '122.jpg', '32.jpg', '161.jpg', '62.jpg', '128.jpg', '212.jpg', '170.jpg', '70.jpg', '173.jpg', '64.jpg', '44.jpg', '146.jpg', '40.jpg', '123.jpg', '23.jpg', '168.jpg', '164.jpg', '81.jpg', '39.jpg', '219.jpg', '47.jpg', '94.jpg', '171.jpg', '43.jpg', '145.jpg', '155.jpg', '3.jpg', '105.jpg', '53.jpg', '133.jpg', '203.jpg', '186.jpg', '208.jpg', '49.jpg', '80.jpg', '34.jpg', '7.jpg', '110.jpg', '91.jpg', '83.jpg', '199.jpg', '204.jpg', '89.jpg', '8.jpg', '13.jpg', '59.jpg', '192.jpg', '131.jpg', '17.jpg', '166.jpg', '72.jpg', '196.jpg', '134.jpg', '185.jpg', '206.jpg', '63.jpg', '54.jpg', '107.jpg', '50.jpg', '174.jpg', '222.jpg', '197.jpg', '169.jpg', '58.jpg', '48.jpg', '88.jpg', '21.jpg', '57.jpg', '160.jpg', '218.jpg', '187.jpg', '191.jpg', '129.jpg', '37.jpg', '157.jpg', '211.jpg', '1.jpg', '52.jpg', '149.jpg', '130.jpg', '151.jpg', '103.jpg', '99.jpg', '116.jpg', '87.jpg', '202.jpg', '74.jpg', '214.jpg', '210.jpg', '121.jpg', '225.jpg', '20.jpg', '188.jpg', '71.jpg', '106.jpg', '14.jpg', '92.jpg', '179.jpg', '102.jpg']\n",
      "['67.jpg', '55.jpg', '124.jpg', '127.jpg', '209.jpg', '84.jpg', '93.jpg', '97.jpg', '115.jpg', '38.jpg', '24.jpg', '119.jpg', '183.jpg', '194.jpg', '10.jpg', '69.jpg', '29.jpg', '19.jpg', '108.jpg', '66.jpg', '73.jpg', '5.jpg', '56.jpg', '112.jpg', '137.jpg', '135.jpg', '65.jpg', '140.jpg', '190.jpg', '224.jpg', '31.jpg', '12.jpg', '35.jpg', '28.jpg', '42.jpg', '125.jpg', '113.jpg', '153.jpg', '51.jpg', '181.jpg', '180.jpg', '76.jpg', '41.jpg', '95.jpg', '138.jpg', '78.jpg', '156.jpg', '26.jpg', '205.jpg', '189.jpg', '126.jpg', '0.jpg', '2.jpg', '77.jpg', '46.jpg', '100.jpg', '109.jpg', '136.jpg', '162.jpg', '90.jpg', '85.jpg', '159.jpg', '150.jpg', '98.jpg', '36.jpg', '132.jpg', '61.jpg', '22.jpg', '147.jpg', '158.jpg', '33.jpg', '11.jpg', '220.jpg', '177.jpg', '6.jpg', '27.jpg', '139.jpg', '216.jpg', '217.jpg', '154.jpg', '4.jpg', '122.jpg', '32.jpg', '161.jpg', '62.jpg', '128.jpg', '212.jpg', '170.jpg', '70.jpg', '173.jpg', '64.jpg', '44.jpg', '146.jpg', '40.jpg', '123.jpg', '23.jpg', '168.jpg', '164.jpg', '81.jpg', '39.jpg', '219.jpg', '47.jpg', '94.jpg', '171.jpg', '43.jpg', '145.jpg', '155.jpg', '3.jpg', '105.jpg', '53.jpg', '133.jpg', '203.jpg', '186.jpg', '208.jpg', '49.jpg', '80.jpg', '34.jpg', '7.jpg', '110.jpg', '91.jpg', '83.jpg', '199.jpg', '204.jpg', '89.jpg', '8.jpg', '13.jpg', '59.jpg', '192.jpg', '131.jpg', '17.jpg', '166.jpg', '72.jpg', '196.jpg', '134.jpg', '185.jpg', '206.jpg', '63.jpg', '54.jpg', '107.jpg', '50.jpg', '174.jpg', '222.jpg', '197.jpg', '169.jpg', '58.jpg', '48.jpg', '88.jpg', '21.jpg', '57.jpg', '160.jpg', '218.jpg', '187.jpg', '191.jpg', '129.jpg', '37.jpg', '157.jpg', '211.jpg', '1.jpg', '52.jpg', '149.jpg', '130.jpg', '151.jpg', '103.jpg', '99.jpg', '116.jpg', '87.jpg', '202.jpg', '74.jpg', '214.jpg', '210.jpg', '121.jpg', '225.jpg', '20.jpg', '188.jpg', '71.jpg', '106.jpg', '14.jpg', '92.jpg', '179.jpg', '102.jpg']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_wts)\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 175\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 130\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m targets_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Iterate over data.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m    131\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    132\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[20], line 45\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     44\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     48\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the dataset\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "image_files = [f\"{name}.jpg\" for name in df['Image'].values]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {'Normal': 0, 'Mitosis': 1}\n",
    "encoded_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split the dataset\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset class with augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define transforms with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Load the pre-trained DenseNet model\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Modify the classifier for binary classification and add dropout\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 1)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Implement Focal Loss instead of weighted BCE\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "criterion = FocalLoss().to(device)\n",
    "\n",
    "# Optimizer with learning rate scheduling\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.sigmoid(outputs).squeeze()\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                preds_binary = torch.round(preds).detach().cpu().numpy()\n",
    "                preds_list.extend(preds_binary)\n",
    "                targets_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = accuracy_score(targets_list, preds_list)\n",
    "            epoch_f1 = f1_score(targets_list, preds_list)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'denseNet_2.pth'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).squeeze()\n",
    "        predicted_labels = torch.round(preds).cpu().numpy()  # Convert to binary labels\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# Map numeric predictions back to label names\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "predicted_labels_text = [label_mapping_inverse[int(pred)] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with predictions\n",
    "test_df['Label'] = predicted_labels_text\n",
    "\n",
    "# Save the updated DataFrame\n",
    "test_df.to_csv('Test_Preds_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Submission Result: \n",
    "\n",
    "Test_Preds_2.csv: 83.87% Accuracy\n",
    "\n",
    "Test_Preds_4.csv: 68%\n",
    "\n",
    "Test_Preds_5.csv: 87.5%\n",
    "\n",
    "Test_Preds_6.csv: 68%\n",
    "\n",
    "Test_Preds_5.csv: 87.5% (model file: DenseNet_2.pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates Made:\n",
    "- Advanced Data Augmentation\n",
    "- Dynamic Learning Rate Adjustment: OneCycleLR scheduler, for more aggressive learning rate management.\n",
    "- Regularization and Fine-tuning Strategy: weight decay in the optimizer for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 0.2236 Acc: 0.6833 F1: 0.6587\n",
      "Val Loss: 0.9096 Acc: 0.7174 F1: 0.1333\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.1922 Acc: 0.7167 F1: 0.6832\n",
      "Val Loss: 360.2284 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.2170 Acc: 0.6944 F1: 0.6584\n",
      "Val Loss: 5679.0238 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.4595 Acc: 0.5722 F1: 0.5217\n",
      "Val Loss: 235.4202 Acc: 0.3043 F1: 0.4667\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.3051 Acc: 0.5278 F1: 0.5029\n",
      "Val Loss: 147.6438 Acc: 0.3043 F1: 0.4667\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.2520 Acc: 0.6611 F1: 0.5960\n",
      "Val Loss: 89.6990 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.1398 Acc: 0.7556 F1: 0.7250\n",
      "Val Loss: 7.1451 Acc: 0.7826 F1: 0.6429\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.2005 Acc: 0.7389 F1: 0.7251\n",
      "Val Loss: 0.1518 Acc: 0.7826 F1: 0.6667\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.2235 Acc: 0.7667 F1: 0.7273\n",
      "Val Loss: 0.1477 Acc: 0.7391 F1: 0.6250\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.1462 Acc: 0.7944 F1: 0.7517\n",
      "Val Loss: 0.1372 Acc: 0.7609 F1: 0.6452\n",
      "Best val F1: 0.666667\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import copy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Load the pre-trained DenseNet model\n",
    "model = models.densenet121(pretrained=True)\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 1)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = self.class_weights[targets.long()]\n",
    "            F_loss *= class_weights\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean', class_weights=class_weights_tensor).to(device)\n",
    "\n",
    "# Optimizer with weight decay for regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Total steps calculation for OneCycleLR\n",
    "EPOCHS = 10\n",
    "train_steps = len(train_loader.dataset) // train_loader.batch_size\n",
    "total_steps = train_steps * EPOCHS\n",
    "\n",
    "# OneCycleLR Scheduler\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, total_steps=total_steps)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for inputs, labels in (train_loader if phase == 'train' else val_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                preds = torch.sigmoid(outputs).detach().cpu()\n",
    "                preds_binary = torch.round(preds).numpy()\n",
    "                preds_list.extend(preds_binary.flatten())\n",
    "                targets_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
    "            epoch_acc = accuracy_score(targets_list, preds_list)\n",
    "            epoch_f1 = f1_score(targets_list, preds_list)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val F1: {:4f}'.format(best_f1))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Proceed with training\n",
    "model = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).squeeze()\n",
    "        predicted_labels = torch.round(preds).cpu().numpy()  # Convert to binary labels\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# Map numeric predictions back to label names\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "predicted_labels_text = [label_mapping_inverse[int(pred)] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with predictions\n",
    "test_df['Label'] = predicted_labels_text\n",
    "\n",
    "# Save the updated DataFrame\n",
    "test_df.to_csv('Test_Preds_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Submission Result: 78.78% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 0.6133 Acc: 0.5722 F1: 0.4539\n",
      "Val Loss: 0.3738 Acc: 0.3043 F1: 0.4667\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.2460 Acc: 0.4722 F1: 0.2963\n",
      "Val Loss: 0.1659 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.1743 Acc: 0.5444 F1: 0.3971\n",
      "Val Loss: 0.1584 Acc: 0.6087 F1: 0.5909\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.1803 Acc: 0.5556 F1: 0.2982\n",
      "Val Loss: 0.1715 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.1955 Acc: 0.4722 F1: 0.4693\n",
      "Val Loss: 0.1775 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.1949 Acc: 0.5167 F1: 0.2927\n",
      "Val Loss: 0.1873 Acc: 0.3043 F1: 0.4667\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.1813 Acc: 0.4944 F1: 0.3358\n",
      "Val Loss: 0.1556 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.1739 Acc: 0.5833 F1: 0.0000\n",
      "Val Loss: 0.1564 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.1740 Acc: 0.5833 F1: 0.0000\n",
      "Val Loss: 0.1605 Acc: 0.6957 F1: 0.0000\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.1733 Acc: 0.5500 F1: 0.0241\n",
      "Val Loss: 0.1648 Acc: 0.6957 F1: 0.0000\n",
      "Best val F1: 0.5909\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "# Load the labels\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "\n",
    "# Assuming 'name' column contains image filenames and 'label' column contains the labels\n",
    "image_files = df['Image'].values\n",
    "image_files = [f\"{name}.jpg\" for name in image_files]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.label_mapping = {\n",
    "            'Normal': 0,\n",
    "            'Mitosis': 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        label_text = self.labels[idx]\n",
    "        label = self.label_mapping[label_text]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure image is RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label_tensor\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained VGG-11 model and modify it for binary classification\n",
    "model = models.vgg11(pretrained=True)\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 1)  # Changing to 1 output feature\n",
    "model = model.to(device)\n",
    "\n",
    "# Implement Focal Loss for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "criterion = FocalLoss().to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for inputs, labels in data_loader:  # Correctly using data_loader here\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float().view(-1, 1)  # Ensure labels are float and correctly shaped\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                preds_binary = torch.sigmoid(outputs.detach()).cpu().numpy() > 0.5\n",
    "                preds_list.extend(preds_binary.flatten())\n",
    "                targets_list.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = accuracy_score(targets_list, preds_list)\n",
    "            epoch_f1 = f1_score(targets_list, preds_list)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Best val F1: {best_f1:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Call train_model function with the defined loaders and parameters\n",
    "model_trained = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, DenseNet has been the only model good enough, so I am now going to focus on boosting that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet Further Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-14 18:36:19,817] A new study created in memory with name: no-name-71b66d11-f7b7-4599-a24c-8d2bcda3b5e2\n",
      "[I 2024-02-14 18:40:27,612] Trial 0 finished with value: 0.64 and parameters: {'lr': 0.004040071357405934, 'dropout_rate': 0.3417845042007459}. Best is trial 0 with value: 0.64.\n",
      "[I 2024-02-14 18:44:36,974] Trial 1 finished with value: 0.0 and parameters: {'lr': 0.04148570617003899, 'dropout_rate': 0.42128452149567486}. Best is trial 0 with value: 0.64.\n",
      "[I 2024-02-14 18:48:50,399] Trial 2 finished with value: 0.6956521739130435 and parameters: {'lr': 0.001692624100690747, 'dropout_rate': 0.2735016071475943}. Best is trial 2 with value: 0.6956521739130435.\n",
      "[I 2024-02-14 18:53:01,727] Trial 3 finished with value: 0.7407407407407408 and parameters: {'lr': 0.00033025381326290735, 'dropout_rate': 0.42137885749872317}. Best is trial 3 with value: 0.7407407407407408.\n",
      "[I 2024-02-14 18:57:12,628] Trial 4 finished with value: 0.6451612903225806 and parameters: {'lr': 4.196535504557435e-05, 'dropout_rate': 0.29706916792388727}. Best is trial 3 with value: 0.7407407407407408.\n",
      "[I 2024-02-14 19:01:21,096] Trial 5 finished with value: 0.5 and parameters: {'lr': 0.00783293226809519, 'dropout_rate': 0.15812443961532074}. Best is trial 3 with value: 0.7407407407407408.\n",
      "[I 2024-02-14 19:05:29,365] Trial 6 finished with value: 0.6285714285714286 and parameters: {'lr': 3.858197610472968e-05, 'dropout_rate': 0.23963181240789325}. Best is trial 3 with value: 0.7407407407407408.\n",
      "[I 2024-02-14 19:09:46,036] Trial 7 finished with value: 0.7407407407407408 and parameters: {'lr': 0.0007907187070687453, 'dropout_rate': 0.13248664936096863}. Best is trial 3 with value: 0.7407407407407408.\n",
      "[I 2024-02-14 19:14:08,682] Trial 8 finished with value: 0.75 and parameters: {'lr': 0.0007182771568338171, 'dropout_rate': 0.28059402905324304}. Best is trial 8 with value: 0.75.\n",
      "[I 2024-02-14 19:18:19,907] Trial 9 finished with value: 0.6086956521739131 and parameters: {'lr': 0.002476368853290585, 'dropout_rate': 0.32046608852063674}. Best is trial 8 with value: 0.75.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value: 0.75\n",
      "Params: \n",
      "lr: 0.0007182771568338171\n",
      "dropout_rate: 0.28059402905324304\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import copy\n",
    "import optuna\n",
    "\n",
    "# Assuming device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming FocalLoss is defined\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Load the dataset\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "image_files = [f\"{name}.jpg\" for name in df['Image'].values]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {'Normal': 0, 'Mitosis': 1}\n",
    "encoded_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split the dataset\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset class with augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define transforms with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    # Model setup\n",
    "    from torchvision.models import densenet121, DenseNet121_Weights\n",
    "    weights = DenseNet121_Weights.DEFAULT\n",
    "    model = densenet121(weights=weights)\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(num_ftrs, 1),\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = FocalLoss().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Training loop simplified for hyperparameter optimization\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(3):  # Reduced number of epochs for faster trials\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)  # Ensure correct shape for binary classification\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).unsqueeze(1)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "                preds_list.extend(preds.flatten())\n",
    "                targets_list.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "        epoch_f1 = f1_score(targets_list, preds_list)\n",
    "        best_f1 = max(best_f1, epoch_f1)\n",
    "\n",
    "    return best_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)  # Reduced number of trials for demonstration\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet with the following updates:\n",
    "- Adding more data augmentation techniques i.e color jittering, vertical flip, image resizing\n",
    "- Adding weighted sampling mechanism to tackle with imbalance\n",
    "- Adding ReduceLROnPlateau learning rate scheduling mechanism in place of StepLR\n",
    "- Adjusting dropout within the model's classifier to prevent overfitting\n",
    "- Adjusting the Focal Loss to incorporate class weights, making the loss function more sensitive to the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 0.2101 Acc: 0.6611 F1: 0.6554\n",
      "Val Loss: 0.2464 Acc: 0.6957 F1: 0.6111\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.1562 Acc: 0.7667 F1: 0.8073\n",
      "Val Loss: 0.1613 Acc: 0.7174 F1: 0.6486\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.1583 Acc: 0.7278 F1: 0.7435\n",
      "Val Loss: 0.1918 Acc: 0.8261 F1: 0.6364\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.1048 Acc: 0.8111 F1: 0.8191\n",
      "Val Loss: 0.3644 Acc: 0.5870 F1: 0.5366\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.1821 Acc: 0.6944 F1: 0.6927\n",
      "Val Loss: 0.1492 Acc: 0.6522 F1: 0.5789\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.1272 Acc: 0.7556 F1: 0.7755\n",
      "Val Loss: 1.0244 Acc: 0.8261 F1: 0.7143\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.1513 Acc: 0.8167 F1: 0.8308\n",
      "Val Loss: 0.2942 Acc: 0.8043 F1: 0.7273\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.1089 Acc: 0.7833 F1: 0.7958\n",
      "Val Loss: 0.1230 Acc: 0.6522 F1: 0.5789\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.1121 Acc: 0.8333 F1: 0.8485\n",
      "Val Loss: 0.3142 Acc: 0.8043 F1: 0.7097\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.0940 Acc: 0.8500 F1: 0.8756\n",
      "Val Loss: 0.1305 Acc: 0.7391 F1: 0.6471\n",
      "Best val F1: 0.727273\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load the dataset\n",
    "labels_csv_path = 'Train.csv'\n",
    "df = pd.read_csv(labels_csv_path)\n",
    "image_files = [f\"{name}.jpg\" for name in df['Image'].values]\n",
    "labels = df['Label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {'Normal': 0, 'Mitosis': 1}\n",
    "encoded_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split the dataset\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(image_files, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset class with augmentation\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, directory, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.image_paths), f\"Index {idx} out of range.\"\n",
    "        img_name = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_path = f\"{self.directory}/{img_name}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Enhanced Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size to allow the model to learn more detailed features\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  # Added vertical flip\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Color variation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Focus on different image parts\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = CustomDataset(train_files, train_labels, 'Training Images', transform=transform)\n",
    "val_dataset = CustomDataset(val_files, val_labels, 'Training Images', transform=transform)\n",
    "\n",
    "# Calculate weights for each class\n",
    "class_sample_counts = np.array([len(np.where(train_labels == t)[0]) for t in np.unique(train_labels)])\n",
    "weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "samples_weights = np.array([weights[t] for t in train_labels])\n",
    "\n",
    "# Create a sampler for weighted sampling\n",
    "sampler = torch.utils.data.WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# Update DataLoader for training set to use the sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=False, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Load the pre-trained DenseNet model\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Modify the classifier for binary classification and add dropout\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.28059402905324304), # adjusted the dropout rate from 0.5 to 0.6\n",
    "    nn.Linear(num_ftrs, 1)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Adjust FocalLoss to use class weights\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            class_weights = self.class_weights[targets.long()]\n",
    "            F_loss *= class_weights\n",
    "\n",
    "        return torch.mean(F_loss) if self.reduction == 'mean' else torch.sum(F_loss)\n",
    "\n",
    "# Calculate class weights and update FocalLoss initialization\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean', class_weights=class_weights_tensor).to(device)\n",
    "\n",
    "# Optimizer with learning rate scheduling\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Optimizer with fine-tuned learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0007182771568338171)  # Adjusted learning rate\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3)  # More dynamic adjustment\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.sigmoid(outputs).squeeze()\n",
    "                    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                preds_binary = torch.round(preds).detach().cpu().numpy()\n",
    "                preds_list.extend(preds_binary)\n",
    "                targets_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = accuracy_score(targets_list, preds_list)\n",
    "            epoch_f1 = f1_score(targets_list, preds_list)\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "            \n",
    "            # if phase == 'val':\n",
    "            #     scheduler.step(epoch_loss)\n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'DN_heavy_aug.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "# Update the DataFrame with predictions\n",
    "test_df['Label'] = predicted_labels_text\n",
    "\n",
    "# Save the updated DataFrame\n",
    "test_df.to_csv('Test_Preds_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).squeeze()\n",
    "        predicted_labels = torch.round(preds).cpu().numpy()  # Convert to binary labels\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# Map numeric predictions back to label names\n",
    "label_mapping_inverse = {0: 'Normal', 1: 'Mitosis'}\n",
    "predicted_labels_text = [label_mapping_inverse[int(pred)] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DataFrame with predictions\n",
    "test_df['Label'] = predicted_labels_text\n",
    "\n",
    "# Save the updated DataFrame\n",
    "test_df.to_csv('Test_Preds_8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
